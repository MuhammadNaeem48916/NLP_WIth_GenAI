{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb54506",
   "metadata": {},
   "source": [
    "# English to French Translator using Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcdd0c",
   "metadata": {},
   "source": [
    "## Import Rwquired Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75b8b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For downloading these Toeknizers from spacy, Uncomment the following lines\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "175bc9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "0.17.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)      # Should be 2.6.0\n",
    "print(torchtext.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d5c4e",
   "metadata": {},
   "source": [
    "## setting up seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "777535c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9405e13",
   "metadata": {},
   "source": [
    "## Need Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7170cffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love coding.', \"J'aime coder.\"), ('The cat is on the mat.', 'Le chat est sur le tapis.'), ('She reads a book.', 'Elle lit un livre.')]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"I love coding.\", \"J'aime coder.\"),\n",
    "    (\"The cat is on the mat.\", \"Le chat est sur le tapis.\"),\n",
    "    (\"She reads a book.\", \"Elle lit un livre.\")\n",
    "]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b7ff2",
   "metadata": {},
   "source": [
    "## preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf458ce2",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06d8f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", \"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0aaf75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<function _spacy_tokenize at 0x000001FA4989C940>, spacy=<spacy.lang.en.English object at 0x000001FA094AA140>)\n",
      "functools.partial(<function _spacy_tokenize at 0x000001FA4989C940>, spacy=<spacy.lang.fr.French object at 0x000001FA094A9EA0>)\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenizer)\n",
    "print(fr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe061b",
   "metadata": {},
   "source": [
    "### Making Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f4c83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making vocab from sentances\n",
    "def build_vocab(sentences, tokenizer):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    specials = [\"<unk>\", \"<pad>\", \"<start>\", \"<end>\"]\n",
    "    vocab_obj = vocab(counter, specials=specials)\n",
    "    vocab_obj.set_default_index(vocab_obj[\"<unk>\"])\n",
    "    \n",
    "    return vocab_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8cb43",
   "metadata": {},
   "source": [
    "### Seperating English and Frenc Sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89d683fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENglisj Sentances are :  ('I love coding.', 'The cat is on the mat.', 'She reads a book.')\n",
      "French Sentances are :  (\"J'aime coder.\", 'Le chat est sur le tapis.', 'Elle lit un livre.')\n"
     ]
    }
   ],
   "source": [
    "eng_sentances , fre_sentances = zip(*data)\n",
    "print(\"ENglisj Sentances are : \", eng_sentances)\n",
    "print(\"French Sentances are : \", fre_sentances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa7c58",
   "metadata": {},
   "source": [
    "### Making English and French Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3772d22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab :  ['<unk>', '<pad>', '<start>', '<end>', 'I', 'love', 'coding', '.', 'The', 'cat', 'is', 'on', 'the', 'mat', 'She', 'reads', 'a', 'book']\n",
      "French Vocab :  ['<unk>', '<pad>', '<start>', '<end>', \"J'\", 'aime', 'coder', '.', 'Le', 'chat', 'est', 'sur', 'le', 'tapis', 'Elle', 'lit', 'un', 'livre']\n"
     ]
    }
   ],
   "source": [
    "eng_vocab = build_vocab(eng_sentances,en_tokenizer)\n",
    "print(\"English Vocab : \", eng_vocab.get_itos()[:50])\n",
    "fre_vocab = build_vocab(fre_sentances,fr_tokenizer)\n",
    "print(\"French Vocab : \", fre_vocab.get_itos()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0425a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk_idx = eng_vocab[\"<unk>\"]\n",
    "# print(unk_idx)\n",
    "# tokens = en_tokenizer(\"some unknown sentence\")\n",
    "# print(tokens)\n",
    "# indices = [eng_vocab[token] if token in eng_vocab.get_stoi() else unk_idx for token in tokens]\n",
    "# print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172caf1e",
   "metadata": {},
   "source": [
    "### Converting To Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13d79916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indices(sentance, tokenizer, vocab):\n",
    "    tokens = tokenizer(sentance)\n",
    "    indices = [vocab[\"<start>\"]] + [vocab[token] for token in tokens] + [vocab['<end>']]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08d15ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 4, 5, 6, 7, 3], [2, 8, 9, 10, 11, 12, 13, 7, 3], [2, 14, 15, 16, 17, 7, 3]]\n",
      "[[2, 4, 5, 6, 7, 3], [2, 8, 9, 10, 11, 12, 13, 7, 3], [2, 14, 15, 16, 17, 7, 3]]\n"
     ]
    }
   ],
   "source": [
    "eng_indices = [convert_to_indices(sent,en_tokenizer, eng_vocab) for sent in eng_sentances]\n",
    "print(eng_indices)\n",
    "fre_indices = [convert_to_indices(sent,fr_tokenizer, fre_vocab) for sent in fre_sentances]\n",
    "print(fre_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5708a",
   "metadata": {},
   "source": [
    "### Padding for Equal length Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d68476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sequences, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "    padded = [seq + [0] * (max_length - len(seq)) for seq in sequences]\n",
    "    return padded, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bf406fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_sequences: [[2, 4, 5, 6, 7, 3, 0, 0, 0], [2, 8, 9, 10, 11, 12, 13, 7, 3], [2, 14, 15, 16, 17, 7, 3, 0, 0]]\n",
      "fre_sequences: [[2, 4, 5, 6, 7, 3, 0, 0, 0], [2, 8, 9, 10, 11, 12, 13, 7, 3], [2, 14, 15, 16, 17, 7, 3, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "eng_sequences, max_len_eng = padding(eng_indices)\n",
    "print(\"eng_sequences:\", eng_sequences)\n",
    "fre_sequences, max_len_fre = padding(fre_indices)\n",
    "print(\"fre_sequences:\", fre_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19036485",
   "metadata": {},
   "source": [
    "### Converting these sequences to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22bd89fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  5,  6,  7,  3,  0,  0,  0],\n",
      "        [ 2,  8,  9, 10, 11, 12, 13,  7,  3],\n",
      "        [ 2, 14, 15, 16, 17,  7,  3,  0,  0]])\n",
      "shape of the English torch.Size([3, 9])\n",
      "tensor([[ 2,  4,  5,  6,  7,  3,  0,  0,  0],\n",
      "        [ 2,  8,  9, 10, 11, 12, 13,  7,  3],\n",
      "        [ 2, 14, 15, 16, 17,  7,  3,  0,  0]])\n",
      " Shape of the french :  torch.Size([3, 9])\n"
     ]
    }
   ],
   "source": [
    "eng_data = torch.tensor(eng_sequences, dtype = torch.long).to(device)\n",
    "print(eng_data)\n",
    "print(\"shape of the English\",eng_data.shape)\n",
    "fre_data = torch.tensor(fre_sequences,dtype = torch.long).to(device)\n",
    "print(fre_data)\n",
    "print(\" Shape of the french : \",fre_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dafefe",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4e360",
   "metadata": {},
   "source": [
    "### Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9db50578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, no_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, no_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.embedding(x)\n",
    "        _,(hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c9b4a",
   "metadata": {},
   "source": [
    "### Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63ebaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embedding_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983d990",
   "metadata": {},
   "source": [
    "### Seq_to_Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6be94858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tar, teacher_forcing=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        tar_len = tar.shape[1]\n",
    "        tar_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, tar_len, tar_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = tar[: ,0]\n",
    "        for i in range(1,tar_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, i] = output\n",
    "            teacher_force = random.random() < teacher_forcing\n",
    "            top1 = output.argmax(1)\n",
    "            input = tar[:, i] if teacher_force else top1\n",
    "        return outputs\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f66c92",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c005538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(eng_vocab)\n",
    "output_dim = len(fre_vocab)\n",
    "embedded_dim = 256\n",
    "hidden_dim = 512\n",
    "no_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "564da5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(input_dim, hidden_dim, embedded_dim, no_layers).to(device)\n",
    "dec = Decoder(output_dim, hidden_dim, embedded_dim, no_layers).to(device)\n",
    "model = Seq2seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4feae8a",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9e716a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = fre_vocab[\"pad\"])\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1745f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, src_data, tar_data, optimizer, creiterion, epochs=10):\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_data, tar_data)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        tar = tar_data[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = creiterion(output, tar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {i+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "98d3ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.8911\n",
      "Epoch 2/10, Loss: 2.7885\n",
      "Epoch 3/10, Loss: 2.6030\n",
      "Epoch 4/10, Loss: 2.2839\n",
      "Epoch 5/10, Loss: 1.9171\n",
      "Epoch 6/10, Loss: 1.5600\n",
      "Epoch 7/10, Loss: 1.2590\n",
      "Epoch 8/10, Loss: 1.0053\n",
      "Epoch 9/10, Loss: 0.7470\n",
      "Epoch 10/10, Loss: 0.6053\n"
     ]
    }
   ],
   "source": [
    "training(model, eng_data, fre_data, optimizer, criterion, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ce3f9",
   "metadata": {},
   "source": [
    "## Making Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0870c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_vocab, fr_vocab, en_tokenizer, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = en_tokenizer(sentence)\n",
    "    indices = [en_vocab[\"<start>\"]] + [\n",
    "        en_vocab[token] if token in en_vocab.get_stoi() else en_vocab[\"<unk>\"]\n",
    "        for token in tokens\n",
    "    ] + [en_vocab[\"<end>\"]]\n",
    "\n",
    "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indices = [fr_vocab[\"<start>\"]]\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == fr_vocab[\"<end>\"]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [fr_vocab.get_itos()[i] for i in trg_indices[1:-1]]\n",
    "    return \" \".join(trg_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bf2dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I love coding.\n",
      "Translation: J' coder .\n"
     ]
    }
   ],
   "source": [
    "# Test translation\n",
    "test_sentence = \"I love coding.\"\n",
    "translation = translate_sentence(test_sentence, model, eng_vocab, fre_vocab, en_tokenizer, device)\n",
    "print(f\"Input: {test_sentence}\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662f3a6",
   "metadata": {},
   "source": [
    "## Evluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d70d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I love coding.\n",
      "Reference: J'aime coder.\n",
      "Predicted: J' coder .\n",
      "\n",
      "Input: The cat is on the mat.\n",
      "Reference: Le chat est sur le tapis.\n",
      "Predicted: Le chat est sur le tapis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_sent, fr_sent in data[:2]:\n",
    "    translation = translate_sentence(en_sent, model, eng_vocab, fre_vocab, en_tokenizer, device)\n",
    "    print(f\"Input: {en_sent}\")\n",
    "    print(f\"Reference: {fr_sent}\")\n",
    "    print(f\"Predicted: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7275a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assumes translate_sentence is already defined as we fixed earlier\n",
    "# Assumes test_data is a list of (src_sentence, trg_sentence) tuples\n",
    "\n",
    "def evaluate_bleu(model, data, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and compute BLEU score.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Seq2Seq model.\n",
    "        data: List of tuples (src_sentence, trg_sentence) as raw strings.\n",
    "        en_vocab: Source vocabulary (English).\n",
    "        fr_vocab: Target vocabulary (French).\n",
    "        en_tokenizer: Tokenizer function for English.\n",
    "        fr_tokenizer: Tokenizer function for French.\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        BLEU score as float.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "\n",
    "    for src_sentence, trg_sentence in tqdm(data, desc=\"Evaluating\"):\n",
    "        pred = translate_sentence(\n",
    "            src_sentence, model, en_vocab, fr_vocab, en_tokenizer, device\n",
    "        )\n",
    "\n",
    "        # Tokenize predicted and actual sentences\n",
    "        pred_tokens = pred.strip().split()\n",
    "        trg_tokens = fr_tokenizer(trg_sentence.strip())\n",
    "\n",
    "        hypotheses.append(pred_tokens)\n",
    "        references.append([trg_tokens])  # Note: list of references\n",
    "\n",
    "    bleu = bleu_score(hypotheses, references)\n",
    "    print(f\"BLEU score = {bleu * 100:.2f}\")\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84a6ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:00<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Example test data\n",
    "test_data = [\n",
    "    (\"I love cats.\", \"J'aime les chats.\"),\n",
    "    (\"She is reading a book.\", \"Elle lit un livre.\"),\n",
    "    (\"We are going to the park.\", \"Nous allons au parc.\"),\n",
    "]\n",
    "\n",
    "bleu = evaluate_bleu(model, test_data, eng_vocab, fre_vocab, en_tokenizer, fr_tokenizer, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
