{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f541f3cb",
   "metadata": {},
   "source": [
    "# Machine Translation for English to French using LSTM based Encoder-Decoder with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f64425",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83d7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm \n",
    "# !python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da120f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49e866",
   "metadata": {},
   "source": [
    "## Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfe8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"E:\\3_MY_Current_Work\\1_Krish_Naik\\3_NLP_GEN_AI\\0.My__Practice\\5.Encoder_Decoder_With_Attention\\english_french_train.csv\"\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417b5ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My name is John</td>\n",
       "      <td>Je m'appelle John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What book are you reading?</td>\n",
       "      <td>Quel livre lis-tu ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have a dog</td>\n",
       "      <td>J’ai un chien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is my mother</td>\n",
       "      <td>C’est ma mère</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s big</td>\n",
       "      <td>C’est grand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      english               french\n",
       "0             My name is John    Je m'appelle John\n",
       "1  What book are you reading?  Quel livre lis-tu ?\n",
       "2                I have a dog        J’ai un chien\n",
       "3           This is my mother        C’est ma mère\n",
       "4                    It’s big          C’est grand"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1543ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['english', 'french'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1499b",
   "metadata": {},
   "source": [
    "## Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8348750",
   "metadata": {},
   "source": [
    "### Tokonizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2fffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "fre_tokenizer = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7884e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text.lower() for tok in tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a89962",
   "metadata": {},
   "source": [
    "### Making Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa90545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, tokenizer, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(tokenize(sentence, tokenizer))\n",
    "\n",
    "    vocab = {'<sos>': 0, '<eos>': 1, '<pad>': 2, '<unk>': 3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq and word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0396c",
   "metadata": {},
   "source": [
    "### Encoding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be0b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocab, tokenizer):\n",
    "    tokens = tokenize(text, tokenizer)\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc42c2",
   "metadata": {},
   "source": [
    "### Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c53521c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text, tgt_text = self.pairs[idx]\n",
    "        src_ids = [self.src_vocab['<sos>']] + encode(src_text, self.src_vocab, eng_tokenizer) + [self.src_vocab['<eos>']]\n",
    "        tgt_ids = [self.tgt_vocab['<sos>']] + encode(tgt_text, self.tgt_vocab, fre_tokenizer) + [self.tgt_vocab['<eos>']]\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95799657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=2)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=2)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b3e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(zip(data['english'], data['french']))\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347b738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 174\n",
      "French Vocabulary Size: 199\n"
     ]
    }
   ],
   "source": [
    "src_vocab = build_vocab([src for src, _ in train_pairs], eng_tokenizer)\n",
    "tgt_vocab = build_vocab([tgt for _, tgt in train_pairs], fre_tokenizer)\n",
    "print(\"English Vocabulary Size:\", len(src_vocab))\n",
    "print(\"French Vocabulary Size:\", len(tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a126d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_pairs, src_vocab, tgt_vocab)\n",
    "test_dataset = TranslationDataset(test_pairs, src_vocab, tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdeaf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822bf89",
   "metadata": {},
   "source": [
    "## Bulding the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428b60b",
   "metadata": {},
   "source": [
    "### The Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f82edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff143b7",
   "metadata": {},
   "source": [
    "### Attention part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f53aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return torch.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd794d",
   "metadata": {},
   "source": [
    "### Decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b95907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim + hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs).permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), context.squeeze(0)), dim=1))\n",
    "        return prediction, hidden, cell, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d450e",
   "metadata": {},
   "source": [
    "### Seq2Seq Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a3af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb54ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]  # <sos>\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c10ed",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7971426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in dataloader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875ba89",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae34afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, trg_vocab, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = ['<sos>'] + tokenize(sentence, eng_tokenizer) + ['<eos>']\n",
    "    src_ids = [src_vocab.get(tok, src_vocab['<unk>']) for tok in tokens]\n",
    "    src_tensor = torch.tensor(src_ids).unsqueeze(1).to(device)\n",
    "    encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [trg_vocab['<sos>']]\n",
    "    attentions = []\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell, attn = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "    trg_vocab_inv = {i: w for w, i in trg_vocab.items()}\n",
    "    return [trg_vocab_inv[idx] for idx in trg_indexes[1:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ce4e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade898be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\naeem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c998abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aad497",
   "metadata": {},
   "source": [
    "### Evaluation using Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d57166d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(pairs, model, src_vocab, tgt_vocab, max_len=50):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for src_text, tgt_text in pairs:\n",
    "        pred_tokens = translate_sentence(model, src_text, src_vocab, tgt_vocab, max_len)\n",
    "        tgt_tokens = tokenize(tgt_text, fre_tokenizer)\n",
    "\n",
    "        predictions.append(pred_tokens)       \n",
    "        targets.append([tgt_tokens])         \n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    bleu = corpus_bleu(targets, predictions, smoothing_function=smoothie)\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62f38f",
   "metadata": {},
   "source": [
    "### Setting the parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eecbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(tgt_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HIDDEN_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee0a95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(HIDDEN_DIM, HIDDEN_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, attn)\n",
    "model = Seq2Seq(enc, dec, src_pad_idx=src_vocab['<pad>'], device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2999544",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c51d86a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 4.6257\n",
      "Epoch 2: Loss = 3.6873\n",
      "Epoch 3: Loss = 3.2235\n",
      "Epoch 4: Loss = 2.7298\n",
      "Epoch 5: Loss = 2.2759\n"
     ]
    }
   ],
   "source": [
    "# Train for 5 epochs\n",
    "for epoch in range(1, 6):\n",
    "    loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01fcfaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 5.44\n"
     ]
    }
   ],
   "source": [
    "# BLEU Evaluation\n",
    "test_data = list(zip(data['english'][:100], data['french'][:100]))  # use small sample\n",
    "bleu = calculate_bleu(test_data, model, src_vocab, tgt_vocab)\n",
    "print(f\"BLEU Score: {bleu*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c10fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Inference:\n",
      "Input: Where is the train station?\n",
      "Output: où est l’ ? ? <eos>\n"
     ]
    }
   ],
   "source": [
    "# Inference Example\n",
    "print(\"Sample Inference:\")\n",
    "sample_text = \"Where is the train station?\"\n",
    "translated = translate_sentence(model, sample_text, src_vocab, tgt_vocab)\n",
    "print(f\"Input: {sample_text}\")\n",
    "print(f\"Output: {' '.join(translated)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
